{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-02-tensorflow-features.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPxeWzEPWkwrXAiPn4fxQCC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujitpal/keras-tutorial-odsc2020/blob/master/03_03_tensorflow_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxpNemFDbbpm"
      },
      "source": [
        "__NOTE: remember to set your runtime type to GPU for this notebook.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYFblvDvaHgD"
      },
      "source": [
        "# Using underlying Tensorflow features\n",
        "\n",
        "In this session, we will look at some interesting Tensorflow features that you should know as a `tf.keras` developer. The \"old\" Keras used to provide a set of `keras.backend` functions that would delegate to the appropriate Tensorflow or Theano function.\n",
        "\n",
        "Because `tf.keras` has only a single backend, the entire Tensorflow library is at your disposal for customizing Keras. The old backend functions are still available via the `tf.keras.backend` package, but chances are very high that it maps 1:1 to an actual TF function.\n",
        "\n",
        "We will cover the following:\n",
        "* tf.data.Dataset\n",
        "* tf.GradientTape and @tf.function\n",
        "* Distributed training using tf.strategy\n",
        "\n",
        "Finally, we will cover some strategies for exploring further and keeping up-to-date on Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQrEjgknbfWl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EmDdwqEK5UO"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "import time\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQlH5MPQodEz"
      },
      "source": [
        "## tf.data.Dataset\n",
        "\n",
        "Tensorflow allows you to create iterators over your data, and Keras can consume these Dataset objects instead of Numpy matrices (such as `(Xtrain, ytrain)` pairs we were using so far).\n",
        "\n",
        "You can create Datasets from various things, [full list is here](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), common ones are:\n",
        "\n",
        "* `Dataset.from_tensor_slices`\n",
        "* `Dataset.from_generator`\n",
        "* `tf.data.TFRecordDataset` objects ([see docs](https://www.tensorflow.org/guide/data#consuming_tfrecord_data)).\n",
        "\n",
        "Dataset is an iterator, so full dataset does not need to fit in memory.\n",
        "\n",
        "General pattern:\n",
        "* Create dataset from input data.\n",
        "* Apply transformations (shuffle, batch, etc) on the data.\n",
        "* Consume the data in streaming manner.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEkKoS0a4FEf"
      },
      "source": [
        "### Dataset.from_tensor_slices()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn8AVq92ADba",
        "outputId": "47986525-bf6d-4ca3-e62c-d324ef87b02a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "(Xtrain, ytrain), _ = keras.datasets.mnist.load_data()\n",
        "\n",
        "Xtrain = Xtrain.reshape(Xtrain.shape[0], 28, 28, 1).astype(np.float32)\n",
        "ytrain = keras.utils.to_categorical(ytrain, num_classes=10)\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((Xtrain, ytrain)).shuffle(len(Xtrain)).batch(32)\n",
        "\n",
        "for Xtrain_b, ytrain_b in train_ds:\n",
        "  print(Xtrain_b.shape, ytrain_b.shape)\n",
        "  break"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "(32, 28, 28, 1) (32, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcW-EPptA8M8"
      },
      "source": [
        "You can also apply various transformations to a dataset. For example, if we want to resize each of these images, we could do something as shown below.\n",
        "\n",
        "Other transformations are __filter__, __enumerate__, __flat_map__, etc.\n",
        "\n",
        "Note that the parameter to `lambda` should mirror the structure of the dataset element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJMy7-ZbGAtO",
        "outputId": "ffe3b92c-822c-4bac-a703-1a5f75585eac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def resize_image(x, y):\n",
        "  image = tf.image.resize(x, (32, 32))\n",
        "  return (image, y)\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((Xtrain, ytrain)).batch(32)\n",
        "resized_ds = train_ds.map(lambda x, y: resize_image(x, y))\n",
        "for X, y in resized_ds:\n",
        "  print(X.shape, y.shape)\n",
        "  break\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 32, 32, 1) (32, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzta-n83KsFR"
      },
      "source": [
        "### Dataset.from_generator()\n",
        "\n",
        "Another option could be to wrap the Keras `ImageDataGenerator` with a Dataset, so that way we have a data stream of augmented images.\n",
        "\n",
        "* Declare a `ImageDataGenerator`\n",
        "* Define an iterator using `ImageDataGenerator.flow`.\n",
        "* Create `Dataset` with `from_generator`, passing the iterator to the function via a lambda.\n",
        "\n",
        "You can call `repeat(n)` on the Dataset to get the benefit of real data augmentation, i.e. repeat(10) will given you 10x the data (original + random augmented according to your configuration of `ImageDataGenerator`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQGEx_fJK9Ui",
        "outputId": "090c9134-4e33-41bc-d284-245723e8d150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "image_augmenter = keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "data_generator = image_augmenter.flow(Xtrain, ytrain, batch_size=BATCH_SIZE)\n",
        "\n",
        "train_ds = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator,\n",
        "    output_types=(tf.float32, tf.float32),\n",
        "    output_shapes=([BATCH_SIZE, 28, 28, 1],\n",
        "                   [BATCH_SIZE, 10])\n",
        ")\n",
        "\n",
        "for X, y in train_ds:\n",
        "  print(X.shape, y.shape)\n",
        "  break\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 28, 28, 1) (32, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtHe56n3MAGV",
        "outputId": "8cc7b911-49da-4899-fe50-6c8ef97c471f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Xtrain.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvH1Z1Tw1FWz"
      },
      "source": [
        "### Keras preprocessing tools\n",
        "\n",
        "In addition, Keras provides the following two convenience functions that can convert images and text files into datasets.\n",
        "\n",
        "* `preprocessing.text_dataset_from_directory` -- [see docs](https://keras.io/api/preprocessing/text/)\n",
        "* `preprocessing.image_dataset_from_directory` -- [see docs](https://keras.io/api/preprocessing/image/)\n",
        "\n",
        "Both depend on a standard directory structure that looks like this:\n",
        "\n",
        "```\n",
        "directory\n",
        "  |\n",
        "  +-- class_label_1\n",
        "  |    |\n",
        "  |    +-- item_1 (image or text, one record)\n",
        "  |    |\n",
        "  |    +-- ...\n",
        "  |    |\n",
        "  |    +-- item_n\n",
        "  +-- class_labels_2\n",
        "  |    |\n",
        "```\n",
        "\n",
        "In general, though, it is possible to create a Dataset from any generator function using `Dataset.from_generator()`, similar to how we built one from `ImageDataGenerator` (see [consuming Python generators](https://www.tensorflow.org/guide/data#consuming_python_generators)) for details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTXDAG4tLM7e"
      },
      "source": [
        "## tf.GradientTape and @tf.function\n",
        "\n",
        "When you need more control over the training loop than calling `fit` on your Keras model, then you can use `tf.GradientTape` which allows for automatic differentiation.\n",
        "\n",
        "The `@tf.function` annotation is meant to put the training loop in \"compiled-mode\", i.e., in non-eager mode.\n",
        "\n",
        "A good example for when you might want more control over the training loop is with a Generative Adversarial Network (GAN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9jhrdrOPDem"
      },
      "source": [
        "### Generative Adversarial Network (GAN)\n",
        "\n",
        "A GAN consists of __two competing networks__ -- a generator and a discriminator - that are trained in an __adversarial manner__.\n",
        "\n",
        "Generator learns to create output that looks real, and discriminator learns to tell real from fake output.\n",
        "\n",
        "As training progresses, generator learns to create output that looks more and more real, and discriminator gets better at telling them apart.\n",
        "\n",
        "The training converges when there is no more improvement in the discriminator.\n",
        "\n",
        "The example here is adapted from the [Tensorflow tutorial on DCGAN](https://www.tensorflow.org/tutorials/generative/dcgan)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1V2Nv-KLJ1B",
        "outputId": "88054c2f-6080-44bd-a997-ad30fd068a21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# NOTE: we are not using labels here, at all\n",
        "(Xtrain, _), (_, _) = keras.datasets.mnist.load_data()\n",
        "\n",
        "Xtrain.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL1hd9kvXnvD"
      },
      "source": [
        "NOISE_DIM = 100\n",
        "BATCH_SIZE = 256\n",
        "NUM_EPOCHS = 50"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8Ovk4xRR78k"
      },
      "source": [
        "# reshape images to add channel and set type to float\n",
        "Xtrain = Xtrain.reshape(Xtrain.shape[0], 28, 28, 1).astype(np.float32)\n",
        "\n",
        "# normalize pixel values to the range [-1, 1]\n",
        "Xtrain = (Xtrain - 127.5) / 127.5\n",
        "\n",
        "# convert to tf Dataset\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(Xtrain).shuffle(len(Xtrain)).batch(BATCH_SIZE)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCztiHC_VtvO"
      },
      "source": [
        "Both the generator and discriminator are convolutional models.\n",
        "\n",
        "The [Conv2DTranspose layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose) is the inverse operation of Convolution, and is sometimes called Deconvolution. Instead of summarizing an area of the image into a smaller but deeper tensor, it expands a small deep tensor into a flatter larger one.\n",
        "\n",
        "So the generator takes a (batch of) random vectors and tries to produce a (batch of) images of shape (None, 28, 28, 1), here None is the batch dimension.\n",
        "\n",
        "The discriminator takes the generated image and tries to predict a number between 0 and 1, which gets converted to 0 (fake) or 1 (true) by the Binary Cross Entropy loss function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgVd0C3dj8oQ"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn5VhyizSQF0"
      },
      "source": [
        "def build_generator():\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Dense(7*7*256, use_bias=False, input_shape=(NOISE_DIM,)))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.LeakyReLU())\n",
        "\n",
        "    model.add(keras.layers.Reshape((7, 7, 256)))\n",
        "    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
        "\n",
        "    model.add(keras.layers.Conv2DTranspose(\n",
        "        128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 7, 7, 128)\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.LeakyReLU())\n",
        "\n",
        "    model.add(keras.layers.Conv2DTranspose(\n",
        "        64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 14, 14, 64)\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.LeakyReLU())\n",
        "\n",
        "    model.add(keras.layers.Conv2DTranspose(\n",
        "        1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 28, 28, 1)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_discriminator():\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
        "                            input_shape=[28, 28, 1]))\n",
        "    model.add(keras.layers.LeakyReLU())\n",
        "    model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    model.add(keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(keras.layers.LeakyReLU())\n",
        "    model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(1))\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I66Rh4FFj_IS"
      },
      "source": [
        "### Loss functions and optimizers\n",
        "\n",
        "The discriminator loss quantifies how well discriminator is able to distinguish real from fake, so it is a combination of the difference between discriminator predictions on real images to an array of 1s, and discriminator predictions on fake (generated) images to an array of 0s.\n",
        "\n",
        "The generator loss quantifies how well it was able to trick the discriminator. So if generator is performing well, it should be able to trick the discriminator completely. So the loss is measured as the difference between the discriminator predictions against fake images generated by generator against an array of 1s.\n",
        "\n",
        "For both generator and discriminator, the optimizer we choose is Adam with learning rate 1e-4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lCMhwvRWXUn"
      },
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DXqDArYkCbg"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "Notice the following:\n",
        "\n",
        "* You have to specify `training=True` for the discriminator and generator. In (high level) Keras, this is set by Keras depending on whether you call `fit` or `predict` (or `evaluate`).\n",
        "* You have to compute the loss by applying the appropriate loss function to the output of the model.\n",
        "* You have to compute and apply the gradient of the loss back into the model using `optimizer.apply_gradients()`.\n",
        "\n",
        "The `train` function just calls the `train_step` function for the given number of epochs and writes out some progress information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1_HBzBwWx3H"
      },
      "source": [
        "@tf.function\n",
        "def train_step(images, generator, discriminator):\n",
        "    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(\n",
        "        zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(\n",
        "        zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyZ5xSf0XCMe"
      },
      "source": [
        "def train(dataset, generator, discriminator, num_epochs):\n",
        "  for epoch in range(num_epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch, generator, discriminator)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ-H4r2YXRBP",
        "outputId": "acbd05ac-78a6-4e87-c56f-bee44c84fb8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "train(train_ds, generator, discriminator, NUM_EPOCHS)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for epoch 1 is 18.251754999160767 sec\n",
            "Time for epoch 2 is 9.9715256690979 sec\n",
            "Time for epoch 3 is 9.979154109954834 sec\n",
            "Time for epoch 4 is 10.042799234390259 sec\n",
            "Time for epoch 5 is 10.095405340194702 sec\n",
            "Time for epoch 6 is 10.172830581665039 sec\n",
            "Time for epoch 7 is 10.211159706115723 sec\n",
            "Time for epoch 8 is 10.259447574615479 sec\n",
            "Time for epoch 9 is 10.304327487945557 sec\n",
            "Time for epoch 10 is 10.365758419036865 sec\n",
            "Time for epoch 11 is 10.400451183319092 sec\n",
            "Time for epoch 12 is 10.427586555480957 sec\n",
            "Time for epoch 13 is 10.449369430541992 sec\n",
            "Time for epoch 14 is 10.475065469741821 sec\n",
            "Time for epoch 15 is 10.502979278564453 sec\n",
            "Time for epoch 16 is 10.544360399246216 sec\n",
            "Time for epoch 17 is 10.553219556808472 sec\n",
            "Time for epoch 18 is 10.584198951721191 sec\n",
            "Time for epoch 19 is 10.602518796920776 sec\n",
            "Time for epoch 20 is 10.598242044448853 sec\n",
            "Time for epoch 21 is 10.636535406112671 sec\n",
            "Time for epoch 22 is 10.638715028762817 sec\n",
            "Time for epoch 23 is 10.648664951324463 sec\n",
            "Time for epoch 24 is 10.680856466293335 sec\n",
            "Time for epoch 25 is 10.700957298278809 sec\n",
            "Time for epoch 26 is 10.71458101272583 sec\n",
            "Time for epoch 27 is 10.697243452072144 sec\n",
            "Time for epoch 28 is 10.6864173412323 sec\n",
            "Time for epoch 29 is 10.711949586868286 sec\n",
            "Time for epoch 30 is 10.708095788955688 sec\n",
            "Time for epoch 31 is 10.71039891242981 sec\n",
            "Time for epoch 32 is 10.71878719329834 sec\n",
            "Time for epoch 33 is 10.728254556655884 sec\n",
            "Time for epoch 34 is 10.721037864685059 sec\n",
            "Time for epoch 35 is 10.728235960006714 sec\n",
            "Time for epoch 36 is 10.735359907150269 sec\n",
            "Time for epoch 37 is 10.745388507843018 sec\n",
            "Time for epoch 38 is 10.726182699203491 sec\n",
            "Time for epoch 39 is 10.727577447891235 sec\n",
            "Time for epoch 40 is 10.738482236862183 sec\n",
            "Time for epoch 41 is 10.743809461593628 sec\n",
            "Time for epoch 42 is 10.7403564453125 sec\n",
            "Time for epoch 43 is 10.774300336837769 sec\n",
            "Time for epoch 44 is 10.774897336959839 sec\n",
            "Time for epoch 45 is 10.77001166343689 sec\n",
            "Time for epoch 46 is 10.773000955581665 sec\n",
            "Time for epoch 47 is 10.773671865463257 sec\n",
            "Time for epoch 48 is 10.776336431503296 sec\n",
            "Time for epoch 49 is 10.788556098937988 sec\n",
            "Time for epoch 50 is 10.783313035964966 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu1_KX8tm311"
      },
      "source": [
        "### What has the generator learned?\n",
        "\n",
        "The generator has learned how to take random input and produce images that look like MNIST digits.\n",
        "\n",
        "We give it a set of random vectors of size `NOISE_DIM=100` and call the generator with `training=False` to get our generated images.\n",
        "\n",
        "As you can see, the generated images kind of look like digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av9lz5QQXaBd",
        "outputId": "36ca12b2-22b8-46cf-f693-54847b377037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "num_examples_to_generate = 8\n",
        "seeds = tf.random.normal([num_examples_to_generate, NOISE_DIM])\n",
        "preds = generator(seeds, training=False)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(preds.shape[0]):\n",
        "  plt.subplot(1, 8, i+1)\n",
        "  plt.imshow(preds[i, :, :, 0], cmap=\"gray\")\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "_ = plt.show()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIFNIV4Uv88N"
      },
      "source": [
        "## Distributed Training\n",
        "\n",
        "Keras (and Tensorflow) allows you to define `strategies` to do distributed training. We will cover training on multiple GPUs on a single machine.\n",
        "\n",
        "This is the most common use case. AWS provides various GPU instances with upto 16 GPUs (P2, P3, G3, G4).\n",
        "\n",
        "Main approaches for distributing training:\n",
        "  * Data Parallelism -- model trains using part of the data on each GPU.\n",
        "  * Model Parallelism -- sub-models trained separately on each GPU.\n",
        "\n",
        "Two requirements:\n",
        "* For fault tolerance, it is recommended to use `ModelCheckpoint` callback to save the model periodically.\n",
        "* Also distributed training needs data to be provided as `tf.data.Datasets`.\n",
        "\n",
        "You can also train across a cluster of GPU enabled machines -- [see this document for more details](https://keras.io/guides/distributed_training/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_RZNWFvyemA"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(Xtrain, ytrain), (Xtest, ytest) = keras.datasets.mnist.load_data()\n",
        "\n",
        "Xtrain = Xtrain.reshape(Xtrain.shape[0], 28, 28, 1).astype(np.float32)\n",
        "Xtest = Xtest.reshape(Xtest.shape[0], 28, 28, 1).astype(np.float32)\n",
        "\n",
        "Xtrain = (Xtrain - 127.5) / 127.5\n",
        "Xtest = (Xtest - 127.5) / 127.5\n",
        "\n",
        "ytrain = keras.utils.to_categorical(ytrain)\n",
        "ytest = keras.utils.to_categorical(ytest)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(Xtrain, ytrain, test_size=0.2, random_state=1)\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(Xtrain)).batch(BATCH_SIZE)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((Xtest, ytest)).batch(BATCH_SIZE)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm5j0GFIzWNA"
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\",\n",
        "                                input_shape=(28, 28, 1)))\n",
        "  model.add(keras.layers.GlobalMaxPooling2D())\n",
        "  model.add(keras.layers.Dense(10))\n",
        "  return model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT9EtYp_wmnl"
      },
      "source": [
        "### Data Parallelism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZicg1bQbUkt",
        "outputId": "a7dfda4f-c810-402a-f1ad-578b56520750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_ckpt = keras.callbacks.ModelCheckpoint(\"/tmp/model_1\", save_best_only=True)\n",
        "\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "with strategy.scope():\n",
        "  model = build_model()\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCgi3CiNdzHe",
        "outputId": "261e7784-3348-4535-d584-5ae2a8f89203",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "%%time\n",
        "model.fit(train_ds, epochs=5, validation_data=val_ds, callbacks=[model_ckpt])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "186/188 [============================>.] - ETA: 0s - loss: 8.3273 - accuracy: 0.1090INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /tmp/model_1/assets\n",
            "188/188 [==============================] - 3s 16ms/step - loss: 8.3281 - accuracy: 0.1088 - val_loss: 8.3169 - val_accuracy: 0.1086\n",
            "Epoch 2/5\n",
            "188/188 [==============================] - 1s 7ms/step - loss: 8.4143 - accuracy: 0.1074 - val_loss: 8.3169 - val_accuracy: 0.1086\n",
            "Epoch 3/5\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 8.4143 - accuracy: 0.1074 - val_loss: 8.3169 - val_accuracy: 0.1086\n",
            "Epoch 4/5\n",
            "188/188 [==============================] - 1s 7ms/step - loss: 8.4143 - accuracy: 0.1074 - val_loss: 8.3169 - val_accuracy: 0.1086\n",
            "Epoch 5/5\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 8.4143 - accuracy: 0.1074 - val_loss: 8.3169 - val_accuracy: 0.1086\n",
            "CPU times: user 9.8 s, sys: 848 ms, total: 10.7 s\n",
            "Wall time: 11.7 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbbc6312c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cz9AuRJ2DCn"
      },
      "source": [
        "### Model Parallelism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKQ7vid2xSwt"
      },
      "source": [
        "input_a = keras.Input(shape=(140, 256))\n",
        "input_b = keras.Input(shape=(140, 256))\n",
        "\n",
        "shared_lstm = keras.layers.LSTM(64)\n",
        "\n",
        "# Process the first sequence on one GPU\n",
        "with tf.device('/gpu:0'):\n",
        "    encoded_a = shared_lstm(input_a)\n",
        "# Process the next sequence on another GPU\n",
        "with tf.device('/gpu:1'):\n",
        "    encoded_b = shared_lstm(input_b)\n",
        "\n",
        "# Concatenate results on CPU\n",
        "with tf.device('/cpu:0'):\n",
        "    merged_vector = keras.layers.concatenate(\n",
        "        [encoded_a, encoded_b], axis=-1)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIr1tUF-bxWn"
      },
      "source": [
        "### Using TPU\n",
        "\n",
        "__NOTE: Please set your Runtime to TPU for this part of the notebook.__\n",
        "\n",
        "TPUs are available on Colab and can make your training go faster. Using TPUs is similar to using GPUs with Data Parallelism from a programming point of view.\n",
        "\n",
        "You create a strategy, and then define and compile the model with the strategy scope.\n",
        "\n",
        "Code is adapted from this [Keras example of Pneumonia X-ray Image Classification with TPU](https://keras.io/examples/vision/xray_classification_with_tpus/).\n",
        "\n",
        "__NOTE:__ In this example, training on TPU is actually slower than training on GPU. This is because the model is small and more time is spent on TPU overhead than gained during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5aP8atFgb6K"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcEhcLVjhCve"
      },
      "source": [
        "# rebuilding the train_ds dataset because we are changing runtimes\n",
        "(Xtrain, ytrain), (Xtest, ytest) = keras.datasets.mnist.load_data()\n",
        "\n",
        "Xtrain = Xtrain.reshape(Xtrain.shape[0], 28, 28, 1).astype(np.float32)\n",
        "Xtest = Xtest.reshape(Xtest.shape[0], 28, 28, 1).astype(np.float32)\n",
        "\n",
        "Xtrain = (Xtrain - 127.5) / 127.5\n",
        "Xtest = (Xtest - 127.5) / 127.5\n",
        "\n",
        "ytrain = keras.utils.to_categorical(ytrain)\n",
        "ytest = keras.utils.to_categorical(ytest)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(Xtrain, ytrain, test_size=0.2, random_state=1)\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(Xtrain)).batch(BATCH_SIZE)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBMzGWLkglMC"
      },
      "source": [
        "# same as build_model above, but need to redefine since we are changing runtimes\n",
        "def build_model_tpu():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\",\n",
        "                                input_shape=(28, 28, 1)))\n",
        "  model.add(keras.layers.GlobalMaxPooling2D())\n",
        "  model.add(keras.layers.Dense(10))\n",
        "  return model"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_5YeWLHch7g",
        "outputId": "0e0a417b-45ed-4faa-ff28-702796bd0867",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print(\"Device:\", tpu.master())\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "except:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"Number of replicas:\", strategy.num_replicas_in_sync)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of replicas: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYu3brb7d9EN"
      },
      "source": [
        "with strategy.scope():\n",
        "  model = build_model_tpu()\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hMdQ49a7XIP",
        "outputId": "0c2d97a2-fc9b-4ae1-b9bc-1d39042d27a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "%%time\n",
        "model.fit(train_ds, epochs=5, validation_data=val_ds)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "188/188 [==============================] - 1s 7ms/step - loss: 11.0962 - accuracy: 0.0986 - val_loss: 11.3968 - val_accuracy: 0.0989\n",
            "Epoch 2/5\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 11.3945 - accuracy: 0.0986 - val_loss: 11.3082 - val_accuracy: 0.0989\n",
            "Epoch 3/5\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 11.3495 - accuracy: 0.0986 - val_loss: 11.2746 - val_accuracy: 0.0989\n",
            "Epoch 4/5\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 11.3129 - accuracy: 0.0986 - val_loss: 11.3042 - val_accuracy: 0.0989\n",
            "Epoch 5/5\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 11.3626 - accuracy: 0.0986 - val_loss: 11.2652 - val_accuracy: 0.0989\n",
            "CPU times: user 6.89 s, sys: 801 ms, total: 7.69 s\n",
            "Wall time: 6.71 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbc53e4cc18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DamGiEHVekhw"
      },
      "source": [
        "__NOTE: reset your runtime type to GPU for the remainder of the notebook, and for the next time you run this notebook.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvm2BjRufIwW"
      },
      "source": [
        "## Doing more with Keras\n",
        "\n",
        "Two aspects:\n",
        "* where to get more information about Keras?\n",
        "* how to keep up-to-date with Keras?\n",
        "\n",
        "### More Info about Keras\n",
        "\n",
        "We've tried to cram all of Keras within 3 hours in this tutorial.\n",
        "\n",
        "As a result, we have not only skipped over most of Deep Learning theory that Keras is built on, but also not covered many Keras classes that are targeted to specific situations.\n",
        "\n",
        "If you want to explore Keras further, check out the [Keras web site](https://keras.io/). It has tons of information, and is well-designed, like Keras. Here you will find:\n",
        "\n",
        "* Two learning tracks, one [for ML engineers](https://keras.io/getting_started/intro_to_keras_for_engineers) and one [for ML researchers](https://keras.io/getting_started/intro_to_keras_for_researchers).\n",
        "* The [Keras API Docs](https://keras.io/api/). I prefer the layout of this one, compared to [tf.keras API docs](https://www.tensorflow.org/api_docs/python/tf/keras) but both should provide similar information, and as we move into the future, perhaps the latter will be more authoritative.\n",
        "* [Keras examples](https://keras.io/examples/) contributed by project members and users, useful for those who learn better from end-to-end examples.\n",
        "\n",
        "If you prefer books, here are some recommendations, from theory heavy to practice heavy.\n",
        "\n",
        "* [Deep Learning](https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/) by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n",
        "* [Deep Learning with Python](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438/) by Francois Chollet.\n",
        "* [Deep Learning with Tensorflow 2 and Keras](https://www.amazon.com/Deep-Learning-TensorFlow-Keras-Regression/dp/1838823417/) by Antonio Gulli, Amita Kapoor, and Sujit Pal.\n",
        "\n",
        "### Keeping up-to-date with Keras\n",
        "\n",
        "* Follow [Francois Chollet (@fchollet) on Twitter](https://twitter.com/fchollet) -- he posts about interesting additions to Keras as they happen, so its a good way to keep up-to-date on whats latest in Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYdtx60uepqc"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}